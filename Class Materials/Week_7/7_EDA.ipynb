{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1fs5WLe77K5"
      },
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "   \n",
        "   [**Introduction**](#Intro1)\n",
        "\n",
        "1. [**Data Cleaning & Wrangling**](#DWrang)\n",
        "\n",
        "    1.1 [**Handling missing data**](#MissD)\n",
        "\n",
        "    1.2 [**Checking and correcting data format**](#Dform)\n",
        "\n",
        "\n",
        "2. [**Exporting Data**](#export)\n",
        "\n",
        "   \n",
        "3. [**Exploratory Data Analysis**](#EDA)\n",
        "\n",
        "    3.1 [**Descriptive statistics & visualization**](#DSV)\n",
        "\n",
        "    3.2 [**Grouping**](#Grouping)\n",
        "\n",
        "    3.3 [**Correlation**](#Corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUeH6hVw8EPf"
      },
      "source": [
        "## Introduction <a name=\"Intro1\"></a>\n",
        "\n",
        "When we are given a data set, first step would be to get to know the data set, its features, and familiarize ourselves with the fields in the data set.\n",
        "\n",
        "Then, we would focus on data cleaning and data wrangling. In __data cleaning__, we try to indetify missing or incorrect values and either remove them from the data set or replace them with meaningful values. In __data wrangling__, we convert the data from initial format to a format that may be better for analysis.\n",
        "\n",
        "Data cleaning and wrangling can also inclue:\n",
        "-  <u>Data normalization</u>: rescaling the values into a range of [0,1]\n",
        "- <u>Data standardization</u>: rescaling data to have a mean of 0 and a standard deviation of 1\n",
        "- <u>Binning</u>: Transformation of a continuous or numerical variable into a categorical feature\n",
        "- <u>Introducing dummy variables</u>: Introducing a variable that takes values of 0 and 1, where the values indicate the presence or absence of something.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFUgDRNo9MML"
      },
      "source": [
        "## 1. Data Cleaning & Wrangling <a name=\"DWrang\"></a>\n",
        "\n",
        "In this notebook, we are going to use Fuel Economy Data Set, which is produced by the Office of Energy Efficiency and Renewable Energy of the U.S. Department of Energy. Fuel economy data are the result of vehicle testing done at the Environmental Protection Agency's National Vehicle and Fuel Emissions Laboratory in Ann Arbor, Michigan, and by vehicle manufacturers with oversight by EPA. This data set can be accessed from here: https://raw.githubusercontent.com/MasoudMiM/ME_364/master/EPA_Green_Vehicle_Guide/Data1.xlsx and a description of the data is provided at this link: https://www.fueleconomy.gov/feg/EPAGreenGuide/GreenVehicleGuideDocumentation.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sbgh4lcQ7yij"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "xlsx = pd.ExcelFile('https://raw.githubusercontent.com/MasoudMiM/ME_364/master/EPA_Green_Vehicle_Guide/Data1.xlsx')\n",
        "df = pd.read_excel(xlsx, 'Sheet1')            # first sheet of the excel file\n",
        "\n",
        "# Data set is now stored in a Pandas's Dataframe\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeegK0bEDW9f"
      },
      "outputs": [],
      "source": [
        "# You can check the size of the data frame\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIZaDm0nfgyA"
      },
      "source": [
        "Once you import the data, you can rename the columns based on your preference. To do that, you can use the following method\n",
        "```python\n",
        "dataframe.rename(columns={Old Name:New Name}, inplace=True)\n",
        "```\n",
        "If you prefer not to change the original data frame, you can omit the `inplace=True` option and put the output of this command to a new data frame.\n",
        "\n",
        "Let's change the name `Veh Class` to `Class`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOqPXsBggVLJ"
      },
      "outputs": [],
      "source": [
        "df.rename(columns={'Veh Class':'Class'}, inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79gZXHYruDYb"
      },
      "source": [
        "<font color=red>__Question (1)__</font>: Change the column named `Underhood ID` to `ID`, `Stnd Description` to `Description`, and `Cert Region` to `Region` in the fuel economy data set. Then show the first five rows of the data set with updated column names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDoT3gAmuIj4"
      },
      "outputs": [],
      "source": [
        "# In-Class Assignment\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIRCFdi6Nb01"
      },
      "source": [
        "## 1.1 Handling missing data <a name=\"MissD\"></a>\n",
        "\n",
        "Most data sets and data gathered from real world have missing and/or incorrect values. Unless the data set is previously inspected carefully and cleaned from any of those, you should expect to have to deal with missing values.\n",
        "\n",
        "Here are some of the pandas' methods that are useful:\n",
        "```python\n",
        ".isnull()   # Finds null values, including NaN in numeric arrays, None or NaN in object arrays, NaT in datetime like\n",
        ".notnull()  # Finds where values are not missing (not null)\n",
        ".value_counts() # Counts of unique values\n",
        ".any() # Return whether any element is True, potentially over an axis (for a dataframe, axis should be given)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Na_4woKrhNSo"
      },
      "outputs": [],
      "source": [
        "df.isnull()  # The output is a boolean value indicating whether the value that is passed into the argument is in fact missing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PacNm3aoSq3f"
      },
      "source": [
        "Let's count the number of missing values in column `Fuel` of our data frame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkljGmvZS2PF"
      },
      "outputs": [],
      "source": [
        "df['Fuel'].isnull().value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxgR4W1fuR0x"
      },
      "source": [
        "<font color=red>__Question (2)__</font>: Find the number of missing values in column `Region`, column `Displ`, and column `Cyl`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7PjhhnfuTfH"
      },
      "outputs": [],
      "source": [
        "# In-Class Assignment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_psSd8r8UcRo"
      },
      "source": [
        "Let's use a for-loop to find the number of missing values in each columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O86QuKAUk06"
      },
      "outputs": [],
      "source": [
        "missing_data=df.isnull()\n",
        "for column in missing_data.columns:\n",
        "    print(column)\n",
        "    print(missing_data[column].value_counts()) \n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anB0ZmYDX26t"
      },
      "source": [
        "Let's also find the rows with missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhAtqTeYX9Ec"
      },
      "outputs": [],
      "source": [
        "df[df.isnull().any(axis=1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyzSWg5wZEcM"
      },
      "source": [
        "How about checking which columns have missing values?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v81_iYP-ZJMq"
      },
      "outputs": [],
      "source": [
        "df.isnull().any(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhagofweuczY"
      },
      "source": [
        "<font color=red>__Question (3)__</font>: Try to find the rows with missing values for the column `Trans`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL7BjTrLudz8"
      },
      "outputs": [],
      "source": [
        "# In-Class Assignment\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C60t2f8rWUG0"
      },
      "source": [
        "If you have missing values, there are multiple ways to deal with them. You can either __drop__ them or __replace__ them.\n",
        "\n",
        "In case of droping, you might have to do either of these:\n",
        "- droping the whole row\n",
        "- droping the whole column\n",
        "\n",
        "In case of replacing, you might want to do either of these:\n",
        "- replacing the value by a reasonable guess\n",
        "- replacing the value by mean\n",
        "- replacing the value by another randomly selected value from that column\n",
        "- replacing the value by interpolation\n",
        "- replacing the value based on other functions\n",
        "\n",
        "Here are some useful methods:\n",
        "```python\n",
        ".dropna()   # Remove missing values\n",
        ".drop()     # Remove rows or columns by specifying label names and corresponding axis\n",
        ".replace()  # Replaces a specific value with a given value\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm8i_HrnbjX5"
      },
      "source": [
        "Looking at all the columns in our data frame, almost all the values in dummy column are `NaN` values. So we can safely remove this column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyxEPE2jb0Hq"
      },
      "outputs": [],
      "source": [
        "df.drop(columns='Dummy Column', inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwOJ1_gmds0u"
      },
      "source": [
        "Let's take a look at the missing values in the columns again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uIhJHD6cVx8"
      },
      "outputs": [],
      "source": [
        "missing_data=df.isnull()\n",
        "for column in missing_data.columns:\n",
        "    print(column)\n",
        "    print (missing_data[column].value_counts())\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V36YCUoTeE1S"
      },
      "source": [
        "Columns `Displ` and `Cyl` has 81 missing values. Let's look at the rows with missing values for those two columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TP9PEceIeheE"
      },
      "outputs": [],
      "source": [
        "df[df['Displ'].isnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRh8XwbKexkW"
      },
      "outputs": [],
      "source": [
        "df[df['Cyl'].isnull()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOl_26Ene_lU"
      },
      "source": [
        "With a quick look, we can realize that those rows belong to electric cars and that's why they have no information regarding the number of engine cylinders (`Cyl`) and engine displacement in liters (`Disp`). That makes sense!\n",
        "\n",
        "We are not going to look into fully electric or hydrogen vehicles. So let's just remove those rows from our data set. To do that, we can use <font color=blue> .dropna() </font> method. This method has multiple options:\n",
        "\n",
        "\n",
        "`axis` determines if rows or columns, which contain missing values, should be removed. _possible values_: 0 or ‘index’, 1 or ‘columns’ (default 0) \n",
        "\n",
        "`how` determines if row or column should be removed from DataFrame, when we have at least one NaN or all NaN. _possible values_: 'any' or 'all'\n",
        "\n",
        "`subset`: Labels along other axis to consider, e.g. if you are dropping rows these would be a list of columns to include.\n",
        "\n",
        "`inplace`: If True, do operation inplace and return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzRr-y7jfjXy"
      },
      "outputs": [],
      "source": [
        "df.dropna(axis=0, how='all', subset=['Displ','Cyl'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n13DksoMjjgS"
      },
      "source": [
        "Looking at the columns again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tz-Z0NVjmHZ"
      },
      "outputs": [],
      "source": [
        "missing_data=df.isnull()\n",
        "for column in missing_data.columns.to_list():\n",
        "    print(column)\n",
        "    print (missing_data[column].value_counts())\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onc3uT8SkyWp"
      },
      "source": [
        "We still have three columns, `Fuel`, `Region`, and `stnd` with at least one null value for each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ-8W344lUK5"
      },
      "outputs": [],
      "source": [
        "# For fuel column\n",
        "df[df['Fuel'].isnull()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abiot16u6TSZ"
      },
      "source": [
        "From what we know, CHEVROLET Silverado 4WD Trail Boss uses Gasoline as fuel. You can actually check that using:\n",
        "\n",
        "```python\n",
        "df[df['Model'].str.contains('CHEVROLET' and 'Silverado')]\n",
        "```\n",
        "\n",
        "So we can fix the dataset by replacing the missing value with `Gasoline`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKJrwkasqm2K"
      },
      "outputs": [],
      "source": [
        "import numpy as np   # We need numpy to use nan\n",
        "\n",
        "df['Fuel'].replace(np.nan, 'Gasoline', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsSdkbiO8LVs"
      },
      "source": [
        "Let's check the row again and see if it is updated now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LY430i34606z"
      },
      "outputs": [],
      "source": [
        "df.loc[[578]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwrk4vUn8RUj"
      },
      "source": [
        "Now, let's take a look at rows with missing values in other columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRXh0xaClzZJ"
      },
      "outputs": [],
      "source": [
        "# For Region column\n",
        "df[df['Region'].isnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nF8yZ6Ol7by"
      },
      "outputs": [],
      "source": [
        "# For Stnd column\n",
        "df[df['Stnd'].isnull()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJrVsPI58354"
      },
      "source": [
        "Since most of the data for this row is missing, we can just remove this row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2au8KHktm1g"
      },
      "outputs": [],
      "source": [
        "df.drop(293, axis=0, inplace=True)\n",
        "df[df['Region'].isnull()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX70QLEwuU5T"
      },
      "source": [
        "At this point, there should not be any NaN value in our data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDD6LyI4uaZa"
      },
      "outputs": [],
      "source": [
        "df.isnull().value_counts()  # or df.isnull().sum().sum() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDkq1WzV2L4h"
      },
      "source": [
        "Finally, we need to reset the index values since we have removed multiple lines from the data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROtVbRL42RHO"
      },
      "outputs": [],
      "source": [
        "df.reset_index(drop=True, inplace=True) # reset the index and drops the old index\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csVqCgrCIWWJ"
      },
      "source": [
        "## 1.2 Checking and correcting data format <a name=\"Dform\"></a>\n",
        "\n",
        "Looking at the data set, we also need to check the data type for each column to make sure they correctly represent the data in that column. Here is a quick guideline for data types between Python, Numpy, and Pandas:\n",
        "\n",
        "![alt text](https://docs.google.com/uc?export=download&id=1YOypE53ETQEUpOXTBU8HVbVtZphHnp-c)\n",
        "\n",
        "To check the data types for all the columns, you can easily use:\n",
        "```python\n",
        "df.dtypes\n",
        "```\n",
        "and make sure that the column has the correct format considering the type of data it is representing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f-Ty74oE55k"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b913uJ9cMWyA"
      },
      "source": [
        "If the data type is not correct, you can change it using <font color='blue'> .astype() </font> attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnKHeNHNIthB"
      },
      "outputs": [],
      "source": [
        "df['Displ']=df['Displ'].astype('float64')\n",
        "df['Cyl']=df['Cyl'].astype('int64')\n",
        "\n",
        "df['Class']=df['Class'].astype('category')\n",
        "df['SmartWay']=df['SmartWay'].astype('category')\n",
        "df['Region']=df['Region'].astype('category')\n",
        "df['Trans']=df['Trans'].astype('category')\n",
        "df['Drive']=df['Drive'].astype('category')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtDTedp41b7_"
      },
      "source": [
        "How about columns `City MPG`, `Hwy MPG`, `Cmb MPG`, and `Comb CO2` ? Let's try converting the format for the column `City MPG`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKI_Esf02kRz"
      },
      "outputs": [],
      "source": [
        "df['City MPG']=df['City MPG'].astype('int64')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ldCkPNX3FpA"
      },
      "source": [
        "Oops! It seems like we have a problem!\n",
        "Let's just see how many of these cars are running on more than one type of fuels.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm-tirfYYtD6"
      },
      "outputs": [],
      "source": [
        "df[ df['City MPG']=='21/49' ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk9eHTuL3Iok"
      },
      "outputs": [],
      "source": [
        "print( df['Fuel'].unique() )    # check to see the possible types of fuels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05QxjsTYJSek"
      },
      "outputs": [],
      "source": [
        "df[df['Fuel']=='Gasoline/Electricity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aG51THTL4mT-"
      },
      "outputs": [],
      "source": [
        "print(df[df['Fuel']=='Gasoline/Electricity'].shape)\n",
        "print(df[df['Fuel']=='Ethanol/Gas'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDVEEcN94ZAF"
      },
      "source": [
        "The easiest way to proceed would be to just remove those cars from our data set since there are not too many of them. If we needed to consider them in our data, we would have to split these columns and create new columns from the character `/`. Take a look at this link to see how can you split a column into two columns by separating the values in that column: https://cmdlinetips.com/2018/11/how-to-split-a-text-column-in-pandas/\n",
        "\n",
        "Here, we take the easy path and take them out of our data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPJy4BMz3xQg"
      },
      "outputs": [],
      "source": [
        "df=df[ df['Fuel']!='Gasoline/Electricity' ]\n",
        "\n",
        "df=df[ df['Fuel']!='Ethanol/Gas' ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HimasySu5o6B"
      },
      "source": [
        "Now, we can convert the formats for those columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRliLH9NNPZm"
      },
      "outputs": [],
      "source": [
        "df['City MPG']=df['City MPG'].astype('int64') \n",
        "df['Hwy MPG']=df['Hwy MPG'].astype('int64')\n",
        "df['Cmb MPG']=df['Cmb MPG'].astype('int64')\n",
        "df['Comb CO2']=df['Comb CO2'].astype('int64')\n",
        "df['Greenhouse Gas Score']=df['Greenhouse Gas Score'].astype('int64')  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnvDmhBY6vvl"
      },
      "source": [
        "Hah! seems like we have another problem. There is at least one cell in `Greenhouse Gas Score` column with character `-` instead of a number. Let's find that row or rows. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28RlGu8b3Sky"
      },
      "outputs": [],
      "source": [
        "df[ df['Greenhouse Gas Score']=='-' ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymmB3Nqt7L8w"
      },
      "source": [
        "To get an estimate about what number we should use for the `Greenhouse Gas Score` for `ACURA RDX`, let's see if we can find some information about this car from the data frame that can help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWcnbGNk37bs"
      },
      "outputs": [],
      "source": [
        "df[ df['Model']=='ACURA RDX' ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ1XvxGK731R"
      },
      "source": [
        "Looking at the above results, we can safely replace the character `-` with Greenhouse Gas Score of 5. So let's do that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYla5Aoo8P0-"
      },
      "outputs": [],
      "source": [
        "df['Greenhouse Gas Score'].replace(to_replace='-', value=5, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pVGHBqC8Px-"
      },
      "outputs": [],
      "source": [
        "df['Greenhouse Gas Score']=df['Greenhouse Gas Score'].astype('int64') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhI8sXJz7pYT"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYLhCdSoEcOD"
      },
      "outputs": [],
      "source": [
        "df.reset_index(drop=True,inplace=True) # reset the index and drops the old index\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00YnQWmdJA8b"
      },
      "source": [
        "## 2. Exporting Data <a name=\"export\"></a>\n",
        "\n",
        "Typically, we do not need to export the data from notbook environment and can continue our data analysis in the same notebook. However, there might be several reasons for exporting the data, including the use of a more efficiet binary format such as Pickle, HDF5, or Parquet. Here is a link to a post about these formats and how they differ based on memory usage and loading/saving time: https://towardsdatascience.com/the-best-format-to-save-pandas-data-414dca023e0d\n",
        "\n",
        "Pandas support different types of file formats and once you are done cleaning and re-arranging your data set, you can export your data using any of pandas commands.\n",
        "\n",
        "![alt text](https://docs.google.com/uc?export=download&id=1JQ7LPiEDnfu_biI7mdfX1rkRalCd31e4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfWQLZ57LU36"
      },
      "source": [
        "If you want to export your data frame as a csv file, use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucAJ23FruzpL"
      },
      "outputs": [],
      "source": [
        "df.to_csv('EPA_2020_Fuel_Economy.csv')\n",
        "\n",
        "from google.colab import files\n",
        "files.download('EPA_2020_Fuel_Economy.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIW5xdkcMFhn"
      },
      "source": [
        "You can also export it as an excel file using:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZevwqhAu6As"
      },
      "outputs": [],
      "source": [
        "df.to_excel('EPA_2020_Fuel_Economy.xlsx', sheet_name='FirstSheet')\n",
        "\n",
        "from google.colab import files\n",
        "files.download('EPA_2020_Fuel_Economy.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj5y3DVbE4hF"
      },
      "source": [
        "## 3.1 Descriptive statistics & visualization <a name=\"DSV\"></a>\n",
        "\n",
        "We look at the basic statistics of each column. The five-number summary is a great start for numerical values:\n",
        "\n",
        "- The sample minimum (smallest observation)\n",
        "\n",
        "- The lower quartile or first quartile\n",
        "\n",
        "- The median (the middle value)\n",
        "\n",
        "- The upper quartile or third quartile\n",
        "\n",
        "- The sample maximum (largest observation)\n",
        "\n",
        "To get summary statistics using Pandas for the whole data frame, we can use `.desrcibe()` method. This method automatically skips variables of type object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd0wcxuEPhFC"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpMNzS_UP-Ar"
      },
      "source": [
        "We can apply the method \"describe\" on the variables of type 'category' as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMdArp8pP_q2"
      },
      "outputs": [],
      "source": [
        "df.describe(include='category')  # you can switch to 'all' to include every column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEGaay9hR-go"
      },
      "source": [
        "`value_counts` is a good way of understanding how many units of each characteristic/variable we have. We can apply the `.value_counts()` on any given column. This method returns the counts of unique values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPiMWccSSBzh"
      },
      "outputs": [],
      "source": [
        "df['SmartWay'].value_counts()   # Try this one as well: df['Region'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTlmAE-nS8UY"
      },
      "source": [
        "You can convert the output to a Dataframe using"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDRYozktTNo3"
      },
      "outputs": [],
      "source": [
        "df_Fuel = df['Fuel'].value_counts().to_frame()\n",
        "df_Fuel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yy9v2GXYVOD"
      },
      "source": [
        "<font color='red'>__Question (4)__</font>: Using the data set we have, find the number of unique values of engine capacity (engine displacement) for all the cars in this dataframe and put the outcomes into a new dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgeEn0gyZXxs"
      },
      "outputs": [],
      "source": [
        "# In-Class Assignment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDQ5Z7lKaq99"
      },
      "source": [
        "Now, let's take a look at some examples of how can we use visualization to explore our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa73XtDqaxlX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw-ozgM6bBDs"
      },
      "source": [
        "Let's find the scatterplot of engine capacity and greenhouse gas score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEfX9F8pbCBl"
      },
      "outputs": [],
      "source": [
        "df.plot(kind='scatter',x='Displ',y='Greenhouse Gas Score',figsize=(10,6),s=70,c='k')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIVLu_uKcsJR"
      },
      "source": [
        "How about city mpg and greenhouse score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYgJXTRPbfGC"
      },
      "outputs": [],
      "source": [
        "df.plot(kind='scatter',x='City MPG',y='Greenhouse Gas Score',figsize=(10,6),s=70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av9QDpa7dFX5"
      },
      "source": [
        "We can also use `.regplot` from seaborn to create a scatter plot and fit a linear regression line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Vz0uKiedO8c"
      },
      "outputs": [],
      "source": [
        "sns.regplot(data=df, x='City MPG', y='Greenhouse Gas Score') # use option fit_reg=False if you don't want to see the line fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFhdOb8Qeec1"
      },
      "source": [
        "<font color='red'>__Question (5)__</font>: Plot a scatter plot for city mpg and combined city/highway CO$_2$ tailpipe emissions. Do you see a strong or a weak linear relationship?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uIQXJ6IfL1V"
      },
      "outputs": [],
      "source": [
        "# In-Class Assignment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugMNq0krh5BY"
      },
      "source": [
        "For categorical variables, we can use box plots. Let's take a look at the distribution of greenhouse gas score between vehicles with different types of drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3QdsHMHiIqO"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.boxplot(x='Drive', y='Greenhouse Gas Score', data=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIJriXHBcUKW"
      },
      "source": [
        "We see some 4WD cars that are outliers with high greenhouse scores. Let's see what are these cars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sogxEKMKuxl2"
      },
      "outputs": [],
      "source": [
        "df_4WD=df[ df['Drive']=='4WD' ]\n",
        "df_4WD_GH=df_4WD[ df_4WD['Greenhouse Gas Score']>8.5 ]\n",
        "df_4WD_GH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeFsM5bNjwdu"
      },
      "source": [
        "<font color='red'>__Question (6)__</font>: Use boxplot to find out which class or classes of `TOYOTA` has the highest median greenhouse gas score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r392yrrjOuN"
      },
      "outputs": [],
      "source": [
        "# In-Class Assignment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaShdR2Fc-1r"
      },
      "source": [
        "We can also look at the distribution of greenhouse gas score values in this dataset using either `hist` plot or `displot`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSJ4f82JyctB"
      },
      "outputs": [],
      "source": [
        "df['Greenhouse Gas Score'].plot(kind='hist', bins=10, color='blue')\n",
        "plt.xlabel('Greenhouse gas score')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjI6GJi5Ozzy"
      },
      "outputs": [],
      "source": [
        "sns.displot(data=df, x='Greenhouse Gas Score', bins=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0dDlwpjLDCN"
      },
      "source": [
        "## 3.2 Grouping <a name=\"Grouping\"></a> \n",
        "\n",
        "In many situations, we may wish to __split__ the data set into groups and do something with those groups. We can group the data based on one or several variables and perform an analysis on the individual groups. In pandas, the `groupby` method groups data by different categories to perform analysis on each group separately.\n",
        "\n",
        "The `groupby` method involves one or more of the following steps:\n",
        "\n",
        "- __Splitting__ the data into groups based on some criteria.\n",
        "\n",
        "- __Applying__ a function to each group independently (aggregation, transformation, filtering)\n",
        "\n",
        "- __Combining__ the results into a data structure.\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://docs.google.com/uc?export=download&id=1Ehy_ZX25XrtHxrv9m7HQfHmSRzZdRi8P)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuWQq9wCzKBZ"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzsf-CjDrIIS"
      },
      "source": [
        "Before we try this method, let's create a new dataframe using some columns from `df` dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-PQFlCurM3Q"
      },
      "outputs": [],
      "source": [
        "df2=df[['Trans','City MPG','Hwy MPG','Air Pollution Score']]\n",
        "df2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m642SfOms_h7"
      },
      "source": [
        "If we want to know what is the average air pollution score for each transmission, we can group by `Trans` and then average them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzoboTSZtrb-"
      },
      "outputs": [],
      "source": [
        "df2.groupby('Trans', as_index=False).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2mhBJ3QveLl"
      },
      "source": [
        "When we split, (i.e., applying `groupby` method) the outcome is a _DataFrameGroupBy_ object under the hood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG9Fmyh5v6mZ"
      },
      "outputs": [],
      "source": [
        "df2.groupby('Trans',as_index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnmRCXdJwE7w"
      },
      "source": [
        "After splitting the data one of the common “apply” steps is to summarize or aggregate the data in some fashion, like mean, sum or median for each group.\n",
        "```python\n",
        "df2.groupby('Trans',as_index=False).mean()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMmk31dqxvWu"
      },
      "source": [
        "In addition, pandas allow us query the grouped object for each query. For that, we use `.get_group()` method. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKDxgf8Hx5GP"
      },
      "outputs": [],
      "source": [
        "df2.groupby('Trans', as_index=False).get_group('SemiAuto-10') # This is equivalent to df2[df2['Trans']=='SemiAuto-10']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LsBG22hzf2w"
      },
      "source": [
        "<font color='red'>__Question (7)__</font>: We want to find out the median City MPG, median Hwy MPG, and median greenhouse gas score for different class of cars.\n",
        "\n",
        "- Create a new dataframe only including columns `Class`, `City MPG`, `Hwy MPG`, and `Greenhouse Gas Score`.\n",
        "\n",
        "- Using `groupby` and aggregation method `.median()` (i.e., split-apply-combine), find the answer to this question.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dXN90UDyqmR"
      },
      "outputs": [],
      "source": [
        "# In-Class Assignment\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXGHMAPX3-pX"
      },
      "source": [
        "We can group the data using two variables, instead of just one. For example, let's group by both `Class` and `Trans`. This groups the dataframe by the unique combinations of these two variables. We can store the results in a new dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_HazvMf4iNg"
      },
      "outputs": [],
      "source": [
        "df_gp = df[['Class','Trans','Greenhouse Gas Score']]\n",
        "df_grouped = df_gp.groupby(['Class','Trans'],as_index=False).mean()\n",
        "df_grouped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04vExlRl5U3l"
      },
      "source": [
        "This grouped data is much easier to visualize when it is made into a pivot table. A pivot table is like an Excel spreadsheet, with one variable along the column and another along the row. We can convert the dataframe to a pivot table using the method `.pivot` to create a pivot table from the groups.\n",
        "\n",
        "In this case, we will leave the `Trans` variable as the rows of the table, and pivot `Class` to become the columns of the table:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNnZ0VFs5nmz"
      },
      "outputs": [],
      "source": [
        "grouped_pivot = df_grouped.pivot(index='Trans', columns='Class')\n",
        "grouped_pivot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z16OHZu6CRp"
      },
      "source": [
        "Often, we won't have data for some of the pivot cells. We can fill these missing cells with the value 0, but any other value could potentially be used as well, depending on how we want to deal with the missing values (look at section 2.1 in this notebook). In case you want to fill all the missing values with 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfnodLAO7KN2"
      },
      "outputs": [],
      "source": [
        "grouped_pivot = grouped_pivot.fillna(0) #fill missing values with 0\n",
        "grouped_pivot.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_hDz_dgwbbs"
      },
      "source": [
        "There is more into `.fillna` method. You can see other capabilities of `.fillna` here: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html\n",
        "\n",
        "One can also use the grouped objects to do some __transformation__ or __filter__ each group based on some condition: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL_Zp90NLdsz"
      },
      "source": [
        "## 3.3 Correlation <a name=\"Corr\"></a>\n",
        "\n",
        "Suppose we are given two variables $x$ and $y$. We say $x$ and $y$ are __correlated__ when the value of $x$ has some prediction power on the value of $y$.\n",
        "\n",
        "The __correlation coefficient__ $r(X,Y)$ is a statistic that measures the degree to which $Y$ is a function of $X$. This value ranges from -1 to 1. The most common method that is used to measure correlation is __Pearson__ correlation.\n",
        "\n",
        "The __Pearson Correlation__ shows the linear relationship between two sets of data. In simple terms, it answers the question, \"Can I draw a line graph to represent the data?\" .\n",
        "\n",
        "The resulting coefficient is a value between -1 and 1 inclusive, where:\n",
        "\n",
        "- __1__ : Fully correlated\n",
        "- __0__ : No relation\n",
        "- __-1__: Anti-correlated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb0A7ti4BzG7"
      },
      "source": [
        "To find the Pearson correlation coefficient, we can use method `.corr` in Pandas. It computes pairwise correlation of columns, excluding NA/null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYn4IaYBCAL9"
      },
      "outputs": [],
      "source": [
        "df.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHZePddEEZL4"
      },
      "source": [
        "We also need to know if our findings are statistically significant. We use the __p-value__ as a threshhold to determine if the statistic meets the criteria ($\\alpha$) for significance. \n",
        "\n",
        "__p < 0.01__: we are 99% certain that the finding is accurate\n",
        "\n",
        "__p < 0.05__: we are 95% certain that the finding is accurate\n",
        "\n",
        "__p < 0.1__ : we are 90% certain that the finding is accurate\n",
        "\n",
        "and etc\n",
        "\n",
        "\n",
        "To find the p-value, we use the <font color='blue'>stats</font> module from <font color='blue'> scipy </font> library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO_aTj42CD5U"
      },
      "outputs": [],
      "source": [
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1-YtwOYJSk8"
      },
      "outputs": [],
      "source": [
        "pearson_coef, p_value = stats.pearsonr(df['Hwy MPG'], df['Greenhouse Gas Score'])\n",
        "print(f\"The Pearson Correlation Coefficient is {pearson_coef:0.3f} with a P-value of P = {p_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwMIrceQJpmz"
      },
      "source": [
        "So since the p-value is < 0.001, we are nearly 100% confident that there is a strong positive relationship between highway miles per gallon and greenhouse gas score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9G0ERy0nJ0sb"
      },
      "outputs": [],
      "source": [
        "pearson_coef, p_value = stats.pearsonr(df['Displ'], df['Air Pollution Score'])\n",
        "print(f\"The Pearson Correlation Coefficient is {pearson_coef} with a P-value of P = {p_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "harXh02hMJSa"
      },
      "source": [
        "So since the p-value is < 0.001, we are nearly 100% confident that there is a weak negative relationship between engine capacity and air pollution score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5GjhGkQNwbT"
      },
      "source": [
        "__Keep in mind__, <font color='purple'>__correlation does not imply causation!__</font>\n",
        "- Correlation: There is a relationship between $x$ and $y$\n",
        "- Causation  : $y$ is influenced by a change in $x$"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "7_EDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}