{"cells":[{"cell_type":"markdown","metadata":{"id":"8HTfFCjZLMGU"},"source":["# Logistic Regression\n","\n","## Table of Contents\n","\n","1. [**Introduction**](#Intro)  \n","    \n","2. [**Model Development Procedure**](#ModelDev)\n","   \n","3. [**Binary Classification**](#BinaryClass)\n","\n","    3.1 [**Model Development**](#BinModelDev)\n","  \n","    3.2 [**Model Evaluation**](#BinModelEval)\n","\n","4. [**Multiclass Classification**](#MultiClass)\n","\n","    4.1 [**Model Development**](#MultModelDev)\n","\n","    4.2 [**Model Evaluation**](#MultModelEval)\n","\n","5. [**Final Comments**](#FinalComments)"]},{"cell_type":"markdown","metadata":{"id":"uaFcMGqILcti"},"source":["## 1 Introduction <a name=\"Intro\"></a>\n","\n","We can apply linear regression to classification problems by converting the class names of training examples to numbers, i.e. probabilities. In order to estimate the class of a data point, we need some sort of guidance on what would be the <u>most probable class</u> for that data point. For this, we use __Logistic Regression__.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Rru65eselOgw"},"source":["## 2 Model Development Procedure <a name=\"ModelDev\"></a>\n","\n","Here are the steps to implement logistic regression in Python using <font color='blue'>scikit-learn</font> library\n","\n","__1.__ Import `LogisticRegression`, `train_test_split`, and `MinMaxScaler` funcions from scikit learn library along with `numpy` library\n","```python\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler  # For normalization\n","import numpy as np\n","```\n","\n","__2.__ Define dependant (target variable) and independent variable (feature) from data set:\n","```python\n","x_data=np.array(df[['feature1','feature2',...]])\n","y_data=df['target variable']\n","```\n","\n","__3.__ Normalize your data using <font color='blue'>MinMaxScaler</font> (Optional but advised)\n","```python\n","MinMaxscaler = MinMaxScaler()  # define min max scaler\n","x_data_scaled = MinMaxscaler.fit_transform(x_data)  # transform data\n","```\n","\n","__4.__ Split the data into train and test sets: `x_train,x_test,y_train,y_test=train_test_split(x_data_scaled,y_data)`\n","\n","\n","\n","__5.__ Create a logistic regression object using the constructor: `lr = LogisticRegression() `\n","\n","\n","__6.__ Use the fit function to fit the model to the training data: `lr.fit(x_train,y_train)`\n","\n","__7.__ Then, make prediction using the test data and training data:\n","```python\n","yhatTest=lr.predict(x_test)\n","yhatTrain=lr.predict(x_train)\n","```\n","\n","__8.__ To see the probability calculated for each class, you can use (Optional)\n","```python\n","yhatTest_prob = lr.predict_proba(x_test)\n","yhatTrain_prob = lr.predict_proba(x_train)\n","```"]},{"cell_type":"markdown","metadata":{"id":"BLOjJDo1ofG2"},"source":["## 3 Binary Classification <a name=\"BinaryClass\"></a>"]},{"cell_type":"markdown","metadata":{"id":"mTIKRUJX6UKK"},"source":["Steel plate faults dataset is provided by Semeion, Research of Sciences of Communication, Via Sersale 117, 00128, Rome, Italy. In this dataset, the faults of steel plates are classified into 7 types. Since it has been donated on October 26,2010, this dataset has been widely used in machine learning for automatic pattern recognition. Types of fault and corresponding numbers of sample are shown in the table below\n","\n","<img src=\"https://docs.google.com/uc?export=download&id=1pw1oJ7plDsTASg_ntI_QSVivQ-tMhlqq\" width=\"500\">\n","\n","\n","The number of samples vary a lot from one category to another. Meanwhile, fault 7 is a special class because it contains all other faults except the first six kinds of fault. In other words, samples in class 7 may have no obvious common characteristics. For every sample, 27 features are recorded, providing evidences for its fault class. All attributes are expressed by integers or real numbers. Detailed information about these 27 independent variables is listed out in the following table.\n","\n","<img src=\"https://docs.google.com/uc?export=download&id=1lAV-mPa2seL9VWkezbaCicnZVwOup2c6\" width=\"500\">\n","\n","\n","Ref: https://www.sciencedirect.com/science/article/pii/S0925231214012193?casa_token=8ZvcrfiUELkAAAAA:Vt2ShomuyzpagA6Su9nSQHzImgti_HHvtK5zuGqgC01It_Xn9UsccPB-5HVtzBonmsYCibDgYQ\n","\n","\n","\n","Ref for the dataset: https://archive.ics.uci.edu/ml/datasets/Steel+Plates+Faults\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1qubuPuuLCkg"},"outputs":[],"source":["import pandas as pd\n","\n","url = ('https://raw.githubusercontent.com/MasoudMiM/ME_364/main/Steel_Plates_Faults/Data.csv')\n","df = pd.read_csv(url,names=['X_Minimum', 'X_Maximum', 'Y_Minimum', 'Y_Maximum', 'Pixels_Areas', 'X_Perimeter',\n","                            'Y_Perimeter', 'Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity',\n","                            'Length_of_Conveyer', 'TypeOfSteel_A300', 'TypeOfSteel_A400', 'Steel_Plate_Thickness',\n","                            'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index', 'Edges_X_Index',\n","                            'Edges_Y_Index', 'Outside_Global_Index', 'LogOfAreas', 'Log_X_Index', 'Log_Y_Index',\n","                            'Orientation_Index', 'Luminosity_Index', 'SigmoidOfAreas', 'Pastry', 'Z_Scratch',\n","                            'K_Scratch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults'])           \n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_72DwPqwXkK"},"outputs":[],"source":["# Check to see if there are missing values in the dataset\n","df.isnull().sum().sum()"]},{"cell_type":"markdown","metadata":{"id":"sH8xNtwZdeH8"},"source":["### 3.1 Model Development <a name=\"BinModelDev\"></a>\n","\n","Step __1__, importing the required libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5NZbQMUqwsX"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler  # For normalization\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"s8asJEeYdjg8"},"source":["Step __2__, defining dependat and independant variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqdM2CJxtQo_"},"outputs":[],"source":["x_data=np.array(df[['X_Minimum', 'X_Maximum', 'Y_Minimum', 'Y_Maximum', 'Pixels_Areas', 'X_Perimeter',\n","             'Y_Perimeter', 'Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity',\n","             'TypeOfSteel_A300', 'TypeOfSteel_A400', 'Steel_Plate_Thickness']])\n","y_data=df['K_Scratch']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"spB3qSqjwcqR"},"outputs":[],"source":["df['K_Scratch'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_XEmnxV3wgHE"},"outputs":[],"source":["print('Target variable distribution:')\n","print( df['K_Scratch'].value_counts() )\n","\n","df['K_Scratch'].value_counts().plot(kind='barh')"]},{"cell_type":"markdown","metadata":{"id":"ysiT8ZECdqgD"},"source":["Step __3__, normalization using MinMaxScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y03ARncPtWd-"},"outputs":[],"source":["MinMaxscaler = MinMaxScaler()  # define min max scaler\n","x_data_scaled = MinMaxscaler.fit_transform(x_data)  # transform data"]},{"cell_type":"markdown","metadata":{"id":"EQiszaJJdyaT"},"source":["Step __4__, spliting the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MVdn2apVtYiH"},"outputs":[],"source":["x_train,x_test,y_train,y_test=train_test_split(x_data_scaled,y_data,test_size=0.3)"]},{"cell_type":"markdown","metadata":{"id":"bmdeEG5ad1Rc"},"source":["Step __5__, creating a logistic regression object"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-aGOEZ3ettH3"},"outputs":[],"source":["lr = LogisticRegression(max_iter=500)"]},{"cell_type":"markdown","metadata":{"id":"Iadq_2YWd_Gq"},"source":["Step __6__, fitting the model (training the  model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFz1ghE_tvL2"},"outputs":[],"source":["lr.fit(x_train,y_train)"]},{"cell_type":"markdown","metadata":{"id":"MZWYSha1eEgi"},"source":["Step __7__, making predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7fHsaEmtw-V"},"outputs":[],"source":["yhatTest=lr.predict(x_test)\n","yhatTrain=lr.predict(x_train)"]},{"cell_type":"markdown","metadata":{"id":"AVeMPDwgDeMy"},"source":["Step __8__, probability for each class\n","\n","`predict_proba`  returns estimates for all classes, ordered by the label of classes, which can be found using `lr.classes_`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubsAgazyR1He"},"outputs":[],"source":["lr.classes_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rSDXQM0rDdLi"},"outputs":[],"source":["print('Order of the classes',lr.classes_)\n","yhatTest_prob = lr.predict_proba(x_test)\n","yhatTest_prob"]},{"cell_type":"markdown","metadata":{"id":"m3ViVglRuFZN"},"source":["### 3.2 Model Evaluation <a name=\"BinModelEval\"></a>\n","\n","To evaluate our model logistic regression model, we use log loss and other previously introduced evaluation metrics.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EwTMzzQhuCb9"},"outputs":[],"source":["from sklearn.metrics import accuracy_score \n","from sklearn.metrics import jaccard_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import log_loss\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import ConfusionMatrixDisplay"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ciPax8aPuLtd"},"outputs":[],"source":["acc_scoreTrain = accuracy_score(y_train,yhatTrain)\n","acc_scoreTest = accuracy_score(y_test,yhatTest)\n","print(f'The accuracy for training data is {acc_scoreTrain:0.3f}')\n","print(f'The accuracy for the test data is {acc_scoreTest:0.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mf9zrExWuNDd"},"outputs":[],"source":["J_scoreTrain = jaccard_score(y_train,yhatTrain)\n","J_scoreTest = jaccard_score(y_test,yhatTest)\n","print(f'Jaccard index for training data is {J_scoreTrain:0.3f}')\n","print(f'Jaccard index for the test data is {J_scoreTest:0.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EY_cEHR6uOxc"},"outputs":[],"source":["F_scoreTrain = f1_score(y_train,yhatTrain)\n","F_scoreTest = f1_score(y_test,yhatTest)\n","print(f'F-score for training data is {F_scoreTrain:0.3f}')\n","print(f'F-score for the test data is {F_scoreTest:0.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fTAdsZLkwOY5"},"outputs":[],"source":["LogLossTrain = log_loss(y_train,yhatTrain)\n","LogLossTest = log_loss(y_test,yhatTest)\n","print(f'Log Loss for training data is {LogLossTrain:0.3f}')\n","print(f'Log loss for the test data is {LogLossTest:0.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7XNXR2wV0peK"},"outputs":[],"source":["print('Confusion matrix for training data')\n","CM_scoreTrain = confusion_matrix(y_train,yhatTrain)   # possible option normalize='true'\n","print(CM_scoreTrain)\n","\n","print(40*'-')\n","\n","print('Confusion matrix for test data')\n","CM_scoreTest = confusion_matrix(y_test,yhatTest)   # possible option normalize='true'\n","print(CM_scoreTest)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ZN3V_lQ01D-"},"outputs":[],"source":["dispTr = ConfusionMatrixDisplay(CM_scoreTrain,display_labels=['No Fault','Fault']) # \n","dispTr.plot()\n","\n","dispTs = ConfusionMatrixDisplay(CM_scoreTest,display_labels=['No Fault','Fault'])\n","dispTs.plot()"]},{"cell_type":"markdown","metadata":{"id":"_BPVBQ7rqSxV"},"source":["## 4 Multiclass Classification <a name=\"MultiClass\"></a>\n","\n","By default, logistic regression cannot be used for classification tasks that have more than two class labels, i.e. multi-class classification. Instead, it requires modification to support multi-class classification problems.\n","\n","One popular approach for adapting logistic regression to multi-class classification problems is to split the multi-class classification problem into multiple binary classification problems and fit a standard logistic regression model on each subproblem.\n","\n","Instead of y=0,1 we will expand our definition so that y=0,1...n. Basically we re-run binary classification multiple times, once for each class."]},{"cell_type":"markdown","metadata":{"id":"EbThrUNZqd_6"},"source":["### 4.1 Model Development <a name=\"MultModelDev\"></a> "]},{"cell_type":"markdown","metadata":{"id":"WVuie15y3S4b"},"source":["We are going to use part of the Fuel Economy Dataset, which is produced by the Office of Energy Efficiency and Renewable Energy of the U.S. Department of Energy. Fuel economy data are the result of vehicle testing done at the Environmental Protection Agency's National Vehicle and Fuel Emissions Laboratory in Ann Arbor, Michigan, and by vehicle manufacturers with oversight by EPA. This dataset can be accessed from here: https://github.com/MasoudMiM/ME_364/blob/main/EPA_Green_Vehicle_Guide/EPA_2020_Fuel_Economy.csv and a description of the data is provided at https://www.fueleconomy.gov/feg/EPAGreenGuide/GreenVehicleGuideDocumentation.pdf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gjh4Ho313RdF"},"outputs":[],"source":["import pandas as pd\n","\n","url = ('https://raw.githubusercontent.com/MasoudMiM/ME_364/main/EPA_Green_Vehicle_Guide/EPA_2020_Fuel_Economy.csv')\n","dfEPA = pd.read_csv(url)           \n","\n","dfEPA.drop(columns='Unnamed: 0',inplace=True)\n","dfEPA.head()"]},{"cell_type":"markdown","metadata":{"id":"1t2DhWbg3X5A"},"source":["Let's develop the model using a target variable that has more than two possible values. In this case, we use SmartWay, which can be 'Yes', 'No', or 'Elite'. This is a multi-class classification."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w3cOF8JD3aOl"},"outputs":[],"source":["dfEPA['SmartWay'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9dhJ7czP3elC"},"outputs":[],"source":["print('Target variable distribution:')\n","print( dfEPA['SmartWay'].value_counts() )\n","\n","dfEPA['SmartWay'].value_counts().plot(kind='barh')"]},{"cell_type":"markdown","metadata":{"id":"00gg5oea36xM"},"source":["Defining dependant and independant variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qx1gpMN83skV"},"outputs":[],"source":["x_dataM=np.array(dfEPA[['City MPG','Hwy MPG']])\n","y_dataM=dfEPA['SmartWay']"]},{"cell_type":"markdown","metadata":{"id":"-nSP6D803zHl"},"source":["Normalizing the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5UbDwULA3zp9"},"outputs":[],"source":["MinMaxscalerM = MinMaxScaler()  # define min max scaler\n","x_data_scaledM = MinMaxscaler.fit_transform(x_dataM)  # transform data "]},{"cell_type":"markdown","metadata":{"id":"r9P-kxs44BTH"},"source":["Splitting the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eO8j4XyV4Avk"},"outputs":[],"source":["x_trainM,x_testM,y_trainM,y_testM=train_test_split(x_data_scaledM,y_dataM,test_size=0.25)"]},{"cell_type":"markdown","metadata":{"id":"hHYMzKNu4MLz"},"source":["Creating a logistic regression object"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LC-ppzcy4LwO"},"outputs":[],"source":["lrM = LogisticRegression(max_iter=500)"]},{"cell_type":"markdown","metadata":{"id":"-d_iITsr4igW"},"source":["Trainig the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_8U249Aa40fX"},"outputs":[],"source":["lrM.fit(x_trainM,y_trainM)"]},{"cell_type":"markdown","metadata":{"id":"KKaAy2rC46VT"},"source":["Making predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uzh5D5yH49uu"},"outputs":[],"source":["yhatTestM=lrM.predict(x_testM)\n","yhatTrainM=lrM.predict(x_trainM)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ggrngURHZmj3"},"outputs":[],"source":["yhatTestM[:2]"]},{"cell_type":"markdown","metadata":{"id":"XbAb7zzo5a-9"},"source":["And finding the probability for each class (optional)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dcIGBG9A5V0F"},"outputs":[],"source":["print('Order of the classes',lrM.classes_)\n","yhatTestM_prob = lrM.predict_proba(x_testM)\n","yhatTestM_prob[:2]"]},{"cell_type":"markdown","metadata":{"id":"ptV0X91iquhv"},"source":["### 4.2 Model Evaluation <a name=\"MultModelEval\"></a>\n","\n","We can use some of the evaluation metrics to assess the performance of our model. Here, let's just look at accuracy and confusion matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Li19Ntv45rU6"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import ConfusionMatrixDisplay"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B720R-Sf6vy1"},"outputs":[],"source":["acc_scoreTrainM = accuracy_score(y_trainM,yhatTrainM)\n","acc_scoreTestM = accuracy_score(y_testM,yhatTestM)\n","print(f'The accuracy for training data is {acc_scoreTrainM:0.3f}')\n","print(f'The accuracy for the test data is {acc_scoreTestM:0.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KWbbYIZq51cg"},"outputs":[],"source":["CM_scoreTrainM = confusion_matrix(y_trainM,yhatTrainM)   # possible option normalize='true'\n","CM_scoreTestM = confusion_matrix(y_testM,yhatTestM)   # possible option normalize='true'\n","\n","dispTrM=ConfusionMatrixDisplay(CM_scoreTrainM, display_labels=lrM.classes_)\n","dispTrM.plot()\n","dispTsM=ConfusionMatrixDisplay(CM_scoreTestM, display_labels=lrM.classes_) \n","dispTsM.plot()"]},{"cell_type":"markdown","metadata":{"id":"xs4Cj5A7cv1A"},"source":["## 5 Final Comments <a name=\"FinalComments\"></a>\n","\n","- Classification is the problem of predicting the right label for a given input and it is different from regression where the labels are continuous variables.\n","\n","- The right way to think about classification is as carving feature space into regions, so that all the points within any given region are destined to be assigned the same label.\n","\n","- Sigmoid function take a real-value input $-\\infty < x< \\infty$ and produces a value ranging over [0,1], .i.e probability.\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"11_Logisic_Regression.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":0}
